{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.0,1.0]), LabeledPoint(0.0, [0.1,0.9]), LabeledPoint(1.0, [1.0,0.0]), LabeledPoint(1.0, [0.8,0.3])]\n"
     ]
    }
   ],
   "source": [
    "# Treinamento de modelo usando SPark (PySpark)\n",
    "\n",
    "#----------------------------------------\n",
    "# Exemplo 1 - Usando a bilbioteca MLLib\n",
    "#----------------------------------------\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc=SparkContext.getOrCreate()\n",
    "\n",
    "# LabeledPoint -> estrutura de dados para alimentar os modelos da biblioteca MLLib\n",
    "# Formato: LabeledPoint(classe/valor_predicao,[caracteristica1,carcteristica2,carcteristica3,...])\n",
    "data=[LabeledPoint(0,[0.0,1.0]),LabeledPoint(0,[0.1,0.9]),LabeledPoint(1,[1.0,0.0]),LabeledPoint(1,[0.8,0.3])]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[LabeledPoint(0.0, [0.0,1.0]), LabeledPoint(0.0, [0.1,0.9]), LabeledPoint(1.0, [1.0,0.0]), LabeledPoint(1.0, [0.8,0.3])]]\n"
     ]
    }
   ],
   "source": [
    "rddData=sc.parallelize(data)\n",
    "print(rddData.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como funciona o treinamento em paralelo?\n",
    "\n",
    "# >> Supondo que dividi a rddData em 2 particoes: rddData=sc.parallelize(data,2) \n",
    "\n",
    "# MASTER: w -> parametro de otimizacao da regressao logistica\n",
    "\n",
    "#iteracao 1\n",
    "#-------------\n",
    "#[\n",
    "# WORKER1: [LabeledPoint(0.0, [0.0,1.0]), LabeledPoint(0.0, [0.1,0.9])] -> some 1 do w\n",
    "# WORKER2: [LabeledPoint(1.0, [1.0,0.0]), LabeledPoint(1.0, [0.8,0.3])] -> subtrai 0,2 do w\n",
    "#]\n",
    "\n",
    "# MASTER: w_ite1=w+1-0.2\n",
    "\n",
    "\n",
    "#iteracao 2\n",
    "#-------------\n",
    "#[\n",
    "# WORKER1: [LabeledPoint(0.0, [0.0,1.0]), LabeledPoint(0.0, [0.1,0.9])] -> some 2 do w_ite1\n",
    "# WORKER2: [LabeledPoint(1.0, [1.0,0.0]), LabeledPoint(1.0, [0.8,0.3])] -> subtrai 0,5 do w_ite1\n",
    "#]\n",
    "\n",
    "# MASTER: w_ite2=w_ite+2-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "lrm = LogisticRegressionWithSGD.train(rddData,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(lrm.predict([0.95,0.12])) # nao paraleliza o comando predict pois nao e rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(lrm.predict([0.01,0.7])) # nao paraleliza o comando predict pois nao e rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1, 0.75], [0.2, 0.5], [0.9, 0.2]]]\n"
     ]
    }
   ],
   "source": [
    "rddTeste=sc.parallelize([[0.1,0.75],[0.2,0.5],[0.9,0.2]])\n",
    "print(rddTeste.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(lrm.predict(rddTeste).collect()) # comando predict executado em paralelo nos workers sobre as particoes da rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "# Exemplo 2 - Usando a blibioteca ML\n",
    "#----------------------------------------\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+---+\n",
      "|  v1| v2|  v3| v4|\n",
      "+----+---+----+---+\n",
      "|0.65|1.0| 0.7|  1|\n",
      "| 0.5|0.7| 0.4|  1|\n",
      "|-0.1|0.0| 0.1|  0|\n",
      "|-0.5|0.2|-0.1|  0|\n",
      "+----+---+----+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "temp_df=spark.createDataFrame([Row(v1=0.65, v2=1.0, v3=0.7, v4=1),Row(v1=0.5,v2=0.7,v3=0.4,v4=1),\n",
    "                              Row(v1=-0.1,v2=0.0,v3=0.1,v4=0),Row(v1=-0.5,v2=0.2,v3=-0.1,v4=0)])\n",
    "\n",
    "print(temp_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "| [0.65,1.0,0.7]|    1|\n",
      "|  [0.5,0.7,0.4]|    1|\n",
      "| [-0.1,0.0,0.1]|    0|\n",
      "|[-0.5,0.2,-0.1]|    0|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData=temp_df.rdd.map(lambda x: (Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\",\"label\"]) \n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingDireto=spark.createDataFrame([Row('features'=[0.65,1.0,0.7], label=1),Row('features'=[0.65, 1.0, 0.7], label=1),Row(features=[0.5,0.7,0.4],label=1),Row(features=[-0.1,0.0,0.1],label=0),Row(features=[-0.5,0.2,-0.1],label=0)]])\n",
    "#trainingDireto.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm=LogisticRegression(maxIter=10)\n",
    "model=lrm.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "|  [0.6,0.9,0.5]|    0|\n",
      "|[-0.4,0.1,-0.7]|    0|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df=spark.createDataFrame([Row(v1=0.6, v2=0.9, v3=0.5, v4=0),Row(v1=-0.4,v2=0.1,v3=-0.7,v4=0)])\n",
    "\n",
    "testData=test_df.rdd.map(lambda x: (Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\",\"label\"]) \n",
    "testData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+--------------------+--------------------+----------+\n",
      "|       features|label|       rawPrediction|         probability|prediction|\n",
      "+---------------+-----+--------------------+--------------------+----------+\n",
      "|  [0.6,0.9,0.5]|    0|[-10.588123992255...|[2.52130407904822...|       1.0|\n",
      "|[-0.4,0.1,-0.7]|    0|[10.9422417586944...|[0.99998230554869...|       0.0|\n",
      "+---------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsTestData=model.transform(testData)\n",
    "predictionsTestData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
