{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Python Fundamentos - Capítulo 6</font>\n",
    "\n",
    "## Download: http://github.com/dsacademybr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream de Dados do Twitter com MongoDB, Pandas e Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando a Conexão com o Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: tweepy in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.0)\nRequirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.11.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\nRequirement already satisfied: PySocks>=1.5.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.6.8)\nRequirement already satisfied: requests>=2.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.19.1)\nRequirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2018.8.24)\nRequirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (1.23)\nRequirement already satisfied: idna<2.8,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2.7)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (3.0.4)\ntensorflow 2.1.0 requires absl-py>=0.7.0, which is not installed.\ntensorflow 2.1.0 requires astor>=0.6.0, which is not installed.\ntensorflow 2.1.0 requires gast==0.2.2, which is not installed.\ntensorflow 2.1.0 requires google-pasta>=0.1.6, which is not installed.\ntensorflow 2.1.0 requires grpcio>=1.8.6, which is not installed.\ntensorflow 2.1.0 requires keras-applications>=1.0.8, which is not installed.\ntensorflow 2.1.0 requires keras-preprocessing>=1.1.0, which is not installed.\ntensorflow 2.1.0 requires opt-einsum>=2.3.2, which is not installed.\ntensorflow 2.1.0 requires protobuf>=3.8.0, which is not installed.\ntensorflow 2.1.0 requires tensorflow-estimator<2.2.0,>=2.1.0rc0, which is not installed.\ntensorflow 2.1.0 requires termcolor>=1.1.0, which is not installed.\ntensorboard 2.1.0 requires absl-py>=0.4, which is not installed.\ntensorboard 2.1.0 requires google-auth-oauthlib<0.5,>=0.4.1, which is not installed.\ntensorboard 2.1.0 requires grpcio>=1.24.3, which is not installed.\ntensorboard 2.1.0 requires markdown>=2.6.8, which is not installed.\ntensorboard 2.1.0 requires protobuf>=3.6.0, which is not installed.\ntwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\ntensorflow 2.1.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.1.0 which is incompatible.\ntensorflow 2.1.0 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\ntensorflow 2.1.0 has requirement wrapt>=1.11.1, but you'll have wrapt 1.10.11 which is incompatible.\ntensorboard 2.1.0 has requirement requests<3,>=2.21.0, but you'll have requests 2.19.1 which is incompatible.\nYou are using pip version 10.0.1, however version 20.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
    }
   ],
   "source": [
    "# Instala o pacote tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os módulos Tweepy, Datetime e Json\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja no manual em pdf como criar sua API no Twitter e configure as suas chaves abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"Hxh3XPIdMuvGi6CRhaJplZVQF\"\n",
    "api_secret= \"ZHrw0QbqqBElOtdWbZK90s6QYTl6BUj24Aad0462W3KHTpZVAh\"\n",
    "access_token = \"846412731926413317-GHRx2c3ANGqJkgBJIb9YJYhnQ4adBY3\"\n",
    "access_token_secret= \"cke8pH4s2Eiexw1ln3N71dCbiAnhZO8mrDaddpVO0a94v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adicione aqui sua Consumer Key\n",
    "consumer_key = \"Hxh3XPIdMuvGi6CRhaJplZVQF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adicione aqui sua Consumer Secret \n",
    "consumer_secret = \"ZHrw0QbqqBElOtdWbZK90s6QYTl6BUj24Aad0462W3KHTpZVAh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adicione aqui seu Access Token\n",
    "access_token = \"846412731926413317-GHRx2c3ANGqJkgBJIb9YJYhnQ4adBY3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adicione aqui seu Access Token Secret\n",
    "access_token_secret = \"cke8pH4s2Eiexw1ln3N71dCbiAnhZO8mrDaddpVO0a94v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando as chaves de autenticação\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando uma classe para capturar os stream de dados do Twitter e \n",
    "# armazenar no MongoDB\n",
    "class MyListener(StreamListener):\n",
    "    def on_data(self, dados):\n",
    "        tweet = json.loads(dados)\n",
    "        created_at = tweet[\"created_at\"]\n",
    "        id_str = tweet[\"id_str\"]\n",
    "        text = tweet[\"text\"]\n",
    "        obj = {\"created_at\":created_at,\"id_str\":id_str,\"text\":text,}\n",
    "        tweetind = col.insert_one(obj).inserted_id\n",
    "        print (obj)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando o objeto mylistener\n",
    "mylistener = MyListener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando o objeto mystream\n",
    "mystream = Stream(auth, listener = mylistener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando a Conexão com o MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importando do PyMongo o módulo MongoClient\n",
    "from pymongo import MongoClient   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando a conexão ao MongoDB\n",
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando o banco de dados twitterdb\n",
    "db = client.twitterdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando a collection \"col\"\n",
    "col = db.tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando uma lista de palavras chave para buscar nos Tweets\n",
    "keywords = ['economia', 'covid-19']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando os Tweets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "source": [
    "# Iniciando o filtro e gravando os tweets no MongoDB\n",
    "mystream.filter(track=keywords)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> Pressione o botão Stop na barra de ferramentas para encerrar a captura dos Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando os Dados no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystream.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'_id': ObjectId('5eb295afdda712d79ae78f05'),\n 'created_at': 'Wed May 06 10:47:06 +0000 2020',\n 'id_str': '1257985235075174400',\n 'text': 'RT @dagenmcdowell: In late March, NY *ordered* nursing homes to accept Covid-19 patients discharged from hospitals. When asked about this d…'}"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Verificando um documento no collection\n",
    "col.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Dados com Pandas e Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# criando um dataset com dados retornados do MongoDB\n",
    "dataset = [{\"created_at\": item[\"created_at\"], \"text\": item[\"text\"],} for item in col.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importando o módulo Pandas para trabalhar com datasets em Python\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando um dataframe a partir do dataset \n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\wesley.neves\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "custon_stop_words = ['nessa','todo','pra','vou','aqui','se','.',',','lá','vcs','pois','\"',':','``']\n",
    "punctuation = list(string.punctuation)\n",
    "lista_stopwords  = set(stopwords.words('portuguese') + punctuation + custon_stop_words+ ['rt', 'via'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(texto):\n",
    "    return re.sub(r'http\\S+', '', texto)\n",
    "\n",
    "\n",
    "def set_tokenize(texto):\n",
    "    return word_tokenize(texto)\n",
    "\n",
    "\n",
    "def remove_stop_Words(texto,lista_stopwords):\n",
    "    palavras =set_tokenize(texto)\n",
    "    palavras_sem_stopwords = [palavra.lower() for palavra in palavras if palavra.lower() not in lista_stopwords]\n",
    "    return (palavras_sem_stopwords)\n",
    "\n",
    "def remove_caracteres_especiais(texto):\n",
    "    punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
    "    remove = ''.join([i for i in texto if not i in punctuations])\n",
    "    remove_1 =  re.sub(r\" ?\\([^)]+\\)\", \"\", remove)\n",
    "    remove_2 =''.join(remove_1).replace(\"'\", \" \")\n",
    "    remove_3 =''.join(remove_2).replace(\"[\", \" \")\n",
    "    remove_4 =''.join(remove_3).replace(\"]\", \" \")\n",
    "    return str(remove_4)   \n",
    "\n",
    "\n",
    "### recuperar o conteudo a ser trabalhado\n",
    "def normaliza_dados(texto,lista_stopwords):\n",
    "    valor = remove_url(texto)\n",
    "    valor_2 = remove_caracteres_especiais(valor)\n",
    "    remove =remove_stop_Words(valor_2,lista_stopwords)\n",
    "\n",
    "    return remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o dataframe\n",
    "df[\"TextoLimpo\"] =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TextoLimpo'] = [\" \".join(normaliza_dados(texto,lista_stopwords)) for texto in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    dagenmcdowell in late march ny ordered nursing...\n1    drkoko28 sudah diumumkan kemarin berikut alur ...\n2    ani delhi police constable whose covid19 test ...\n3    bjp4india covid19 की वजह से विदेशों में फंसे भ...\n4    drmahreenbhutto get well soon rashid bhai sind...\nName: TextoLimpo, dtype: object"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "df[\"TextoLimpo\"][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Usando o método CountVectorizer para criar uma matriz de documentos\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(df.TextoLimpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                       created_at  \\\n0  Wed May 06 10:47:06 +0000 2020   \n1  Wed May 06 10:47:07 +0000 2020   \n\n                                                text  \\\n0  RT @dagenmcdowell: In late March, NY *ordered*...   \n1  RT @dr_koko28: Sudah diumumkan kemarin berikut...   \n\n                                          TextoLimpo  \\\n0  dagenmcdowell in late march ny ordered nursing...   \n1  drkoko28 sudah diumumkan kemarin berikut alur ...   \n\n                                       ConteudoLimpo  \n0  dagenmcdowell in late march ny ordered nursing...  \n1  drkoko28 sudah diumumkan kemarin berikut alur ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>text</th>\n      <th>TextoLimpo</th>\n      <th>ConteudoLimpo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wed May 06 10:47:06 +0000 2020</td>\n      <td>RT @dagenmcdowell: In late March, NY *ordered*...</td>\n      <td>dagenmcdowell in late march ny ordered nursing...</td>\n      <td>dagenmcdowell in late march ny ordered nursing...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Wed May 06 10:47:07 +0000 2020</td>\n      <td>RT @dr_koko28: Sudah diumumkan kemarin berikut...</td>\n      <td>drkoko28 sudah diumumkan kemarin berikut alur ...</td>\n      <td>drkoko28 sudah diumumkan kemarin berikut alur ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importando o módulo Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           word  count\n0       covid19   2191\n1           the   1416\n2            of    826\n3            to    761\n4            in    727\n5           and    532\n6            la    427\n7            is    329\n8            on    321\n9            el    303\n10           en    264\n11         from    253\n12          has    243\n13          are    234\n14         that    211\n15         with    204\n16          was    202\n17           we    189\n18         this    188\n19           at    180\n20          you    152\n21          del    146\n22           by    145\n23        cases    140\n24       deaths    134\n25           be    133\n26         have    129\n27           it    127\n28         will    124\n29          new    124\n30          our    114\n31           he    108\n32          los    108\n33           un    106\n34           di    105\n35  coronavirus    105\n36     patients    105\n37       people    105\n38          amp    102\n39        about    101\n40           an    101\n41         very    100\n42          las     95\n43          all     93\n44          not     90\n45         more     89\n46           le     89\n47           us     85\n48           if     85\n49           uk     83",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>covid19</td>\n      <td>2191</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the</td>\n      <td>1416</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>of</td>\n      <td>826</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>to</td>\n      <td>761</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>in</td>\n      <td>727</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>and</td>\n      <td>532</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>la</td>\n      <td>427</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>is</td>\n      <td>329</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>on</td>\n      <td>321</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>el</td>\n      <td>303</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>en</td>\n      <td>264</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>from</td>\n      <td>253</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>has</td>\n      <td>243</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>are</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>that</td>\n      <td>211</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>with</td>\n      <td>204</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>was</td>\n      <td>202</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>we</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>this</td>\n      <td>188</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>at</td>\n      <td>180</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>you</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>del</td>\n      <td>146</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>by</td>\n      <td>145</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>cases</td>\n      <td>140</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>deaths</td>\n      <td>134</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>be</td>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>have</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>it</td>\n      <td>127</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>will</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>new</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>our</td>\n      <td>114</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>he</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>los</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>un</td>\n      <td>106</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>di</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>coronavirus</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>patients</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>people</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>amp</td>\n      <td>102</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>about</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>an</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>very</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>las</td>\n      <td>95</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>all</td>\n      <td>93</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>not</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>more</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>le</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>us</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>if</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>uk</td>\n      <td>83</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "# Contando o número de ocorrências das principais palavras em nosso dataset\n",
    "word_count = pd.DataFrame(cv.get_feature_names(), columns=[\"word\"])\n",
    "word_count[\"count\"] = count_matrix.sum(axis=0).tolist()[0]\n",
    "word_count = word_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "word_count[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=\"http://facebook.com/dsacademybr\">facebook.com/dsacademybr</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda12a05f8498994746951df3cf97509cd1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}